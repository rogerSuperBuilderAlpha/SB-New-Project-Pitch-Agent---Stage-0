# Section-by-Section Examples: Good vs Needs Work

> This guide provides concrete examples of what works and what needs improvement for each section of the S0 pitch.

---

## 1. One-liner Examples

### Format A: Business Structure

#### ✅ Great Example
"For middle-school math teachers who struggle to close fraction gaps in mixed-proficiency classes, we help them achieve two times mastery velocity within thirty days by generating personalized practice and quick checks aligned to the district scope and sequence."

**Why it works**:
- Specific ICP (middle-school math teachers, not "all educators")
- Concrete pain (close fraction gaps in mixed-proficiency classes)
- Measurable outcome (two times mastery velocity within thirty days)
- Clear approach (personalized practice and quick checks aligned to standards)

---

#### ⚠️ Needs Work Example
"For educators who want to improve student learning, we help them get better results by using AI-powered personalized learning."

**Why it needs work**:
- Vague ICP ("educators" - what grade, what subject, what setting?)
- Generic pain ("want to improve learning" - everyone wants this)
- Unmeasurable outcome ("better results" - how much better? by when?)
- Buzzword approach ("AI-powered personalized learning" - what specifically?)

**How to fix it**: Narrow the ICP (grade band, subject), specify the learning gap, quantify the improvement (two times faster, twenty-point gain), and describe the concrete mechanism.

---

### Format B: Movie Pitch

#### ✅ Great Example
"It's like Khan Academy meets Quizlet, but with auto-generated intervention plans based on error patterns and real-time alignment to district pacing guides."

**Why it works**:
- Uses recognizable products (Khan Academy, Quizlet)
- Specific differentiators (auto-generated intervention plans from error patterns, alignment to district pacing)
- Clear value (addresses gaps the other products do not)

---

#### ⚠️ Needs Work Example
"It's like EdTech Platform X meets Learning App Y, but with better UX and more engaging content."

**Why it needs work**:
- Unknown products (EdTech Platform X, Learning App Y - not widely recognized)
- Vague differentiators ("better UX," "more engaging" - compared to what? how?)
- No specific value proposition

**How to fix it**: Use products your audience knows (Duolingo, Google Classroom, Kahoot). Make differentiators concrete ("real-time progress tracking for parents" not "better engagement").

---

## 2. ICP (Ideal Customer Profile) Examples

### ✅ Great Example

**School category**: Alpha Middle School (pilot), Beta Academy Network charter schools (grades six through eight)

**Inclusion rules**:
- **Grade band and subject**: Grades six through eight, math (fractions and pre-algebra)
- **School type**: Charter schools with one-to-one Chromebook programs
- **Workflow constraints**: Twenty to thirty minute intervention blocks twice per week
- **Tools**: Google Classroom required, uses Illustrative Math curriculum
- **Minimum scale**: At least one hundred students per grade level

**Exclusion rules**:
- Elementary grades K through five (different foundational approach needed)
- High school algebra two and above (different gaps)
- Schools without one-to-one devices or reliable internet
- Districts prohibiting third-party tools
- Advanced programs where fraction fluency is prerequisite
- Alternative schools with attendance below sixty percent

**Why it works**:
- School category explicitly stated
- Inclusion rules are specific and measurable
- Exclusions make it clear where approach will NOT work
- Two reviewers would classify leads identically

---

### ⚠️ Needs Work Example

**School category**: [Missing]

**Inclusion rules**:
- Teachers who care about student success
- Schools that want to improve learning outcomes
- Any grade level or subject

**Exclusion rules**: [None listed]

**Why it needs work**:
- No school category specified
- Inclusion rules too vague ("teachers who care" - that's everyone)
- No specific grade, subject, or setting
- No exclusions (suggests lack of focus)
- Would cause disagreement between reviewers

**How to fix it**: Name specific schools. Define grade band, subject, school type, workflow, and tools. List what is OUT of scope. Make classification unambiguous.

---

## 3. Strategic Narrative Examples

### ✅ Great Example

**Why Now**: Adaptive content generation and auto-scoring have become good enough and cheap enough to personalize at the student level. Three years ago this would have required human tutors.

**Status Quo**: Static worksheets force teachers to spend hours finding materials for each proficiency level. Remediation cycles take weeks because teachers cannot quickly identify error patterns.

**Promised Land**: Students progress twice as fast on priority fraction standards while teachers recover meaningful prep time. Instead of two months to reach proficiency, students get there in one month.

**Proof**: We ran a four-week pilot with two math teachers at Alpha Middle School where students progressed one point four times faster on released items. Our curriculum advisor spent twelve years designing math interventions for charter networks.

**Why it works**:
- Why Now is external (technology cost/quality shift, not "we have users")
- Status Quo is concrete with specific problems
- Promised Land is measurable (twice as fast, one month vs two months)
- Proof is specific (pilot data, advisor credentials, not generic "we have expertise")

---

### ⚠️ Needs Work Example

**Why Now**: We have traction and users are excited about our product.

**Status Quo**: Education is broken and teachers are overwhelmed.

**Promised Land**: We will revolutionize learning and make education better for everyone.

**Proof**: Our team is passionate about education and we have great advisors.

**Why it needs work**:
- Why Now is internal ("we have traction" - not a market timing signal)
- Status Quo is vague ("education is broken" - too broad, no specifics)
- Promised Land has no metrics ("better" - how much? by when?)
- Proof is hand-wavy ("passionate," "great advisors" - no specifics)

**How to fix it**: Identify external forces (technology shifts, policy changes). Describe concrete current problems. Quantify improvement. Provide specific evidence (pilot results, domain access, named advisors with credentials).

---

## 4. Distribution and Platform Examples

### ✅ Great Example

**Platform**: Web-based tool accessible via any browser, with Google Classroom signin integration.

**Day-one access**: Teachers add FractionFlow as an assignment in Google Classroom. Students click the link, sign in with their school Google account, and start their first diagnostic. No app install, no new passwords.

**Integration blockers**: None. Alpha Middle School already uses Google Classroom for all assignments and has whitelisted our domain for single sign-on.

**Why it works**:
- Platform is specific (web-based, Google signin)
- Access path is concrete step-by-step
- Blockers addressed (already whitelisted)
- Clear this will work on day one

---

### ⚠️ Needs Work Example

**Platform**: Cross-platform mobile-first solution in the cloud.

**Day-one access**: We will work with the school to get students onboarded.

**Integration blockers**: We will handle any technical issues as they come up.

**Why it needs work**:
- Platform is vague buzzwords ("cross-platform mobile-first cloud" - what does this mean?)
- No concrete access path (how exactly will students start using it?)
- No clarity on blockers ("will handle issues" - what issues? how long?)

**How to fix it**: Name the specific platform (iOS app, web browser, Timeback module). Describe exact steps for student access. Identify and address potential blockers before launch.

---

## 5. Measurable Outcome Examples

### ✅ Great Example

**Metric**: Mastery speed on priority fraction standards (addition, subtraction, multiplication, division with unlike denominators)

**Target**: Students reach proficiency twice as fast (in fifteen days instead of thirty days)

**Time window**: Within thirty days of starting the program

**Measurement plan**: Pre-tests and post-tests using released state items, comparing time-to-proficiency against previous semester baseline data across sixty students in three intervention sections. Includes forty-eight-hour delayed retention check.

**Thresholds**:
- **Green**: Fifteen days or less to proficiency with at least seventy-five percent delayed retention
- **Yellow**: Sixteen to twenty-two days or delayed retention below seventy percent
- **Red**: Twenty-three days or more or delayed retention below sixty-five percent

**Why it works**:
- Metric is learning-focused (mastery, not engagement)
- Target is numeric and specific (twice as fast, fifteen vs thirty days)
- Time window is clear and short (thirty days)
- Measurement plan is auditable (specific instruments, sample size, baseline comparison, delayed check)
- Thresholds are unambiguous (clear green/yellow/red)

---

### ⚠️ Needs Work Example

**Metric**: Student engagement and learning outcomes

**Target**: Significant improvement

**Time window**: Over the school year

**Measurement plan**: We will track usage and survey teachers

**Thresholds**: We will know it is working if teachers and students like it

**Why it needs work**:
- Metric mixes engagement (input) with learning (outcome) and is vague
- Target is not numeric ("significant improvement" - how much?)
- Time window is too long (school year - need signal in thirty to sixty days)
- Measurement plan is not auditable (surveys are subjective, "usage" does not measure learning)
- Thresholds are opinion-based ("if teachers like it" - not learning metrics)

**How to fix it**: Choose one clear learning metric (mastery speed, retention, growth). Set numeric target (two times, twenty-point gain). Define time window (fourteen to thirty days). Use objective instruments (pre/post tests with released items, delayed checks). Set numeric thresholds.

---

## 6. Assumptions and Risks Examples

### ✅ Great Example

**Assumption**: Students will complete approximately twenty minutes per day with minimal teacher prompting.

**Test**: In week one we will require a three-day engagement streak and measure daily drop-off rates. If completion falls below seventy-five percent, we will add streak cues and reduce initial session length to twelve minutes. We will check engagement metrics daily and adjust by end of week one.

**Why it works**:
- Assumption is specific and testable
- Test has clear metric (seventy-five percent completion)
- Test has timeframe (week one, daily checks)
- Mitigation is concrete (streak cues, shorter sessions)
- Owner and timing are clear

---

### ⚠️ Needs Work Example

**Assumption**: Students will be engaged.

**Test**: We will monitor engagement.

**Why it needs work**:
- Assumption is vague ("engaged" - what does this mean? how much? how often?)
- Test has no metric (how will you measure?)
- Test has no timeframe (when will you check?)
- No mitigation plan (what if engagement is low?)

**How to fix it**: Define the assumption with numbers (twenty minutes per day, three-day streak). Specify the test metric (seventy-five percent completion rate). Set timeframe (week one, daily checks). Include mitigation (if below threshold, do X).

---

## Common Pitfalls and How to Avoid Them

### Pitfall 1: Feature Soup
**Bad**: "We offer personalized learning paths, gamification, real-time feedback, adaptive assessments, teacher dashboards, parent portals, and AI-powered content generation."

**Good**: "We generate personalized fraction practice aligned to district pacing guides. Students get immediate feedback and teachers see intervention plans based on error patterns."

**Lesson**: Focus on the core value, not a list of features.

---

### Pitfall 2: Engagement as Outcome
**Bad**: "Our primary outcome is increased student engagement, measured by time on platform and number of sessions."

**Good**: "Our primary outcome is mastery speed on priority standards, measured by days from diagnostic to proficiency on released items, with a forty-eight-hour delayed retention check."

**Lesson**: Engagement is an input. Learning is the outcome.

---

### Pitfall 3: Vague Timeframes
**Bad**: "Over time, students will show improvement."

**Good**: "Within thirty days, students will reach proficiency in half the time compared to baseline."

**Lesson**: Always specify timeframe (fourteen days, thirty days, one unit).

---

### Pitfall 4: Unmeasurable Claims
**Bad**: "Students will love learning and achieve their full potential."

**Good**: "Students will score at least eighty percent on post-tests using released state items and maintain at least seventy-five percent accuracy on forty-eight-hour delayed checks."

**Lesson**: Use objective, auditable metrics, not aspirations.

---

### Pitfall 5: Internal Why Now
**Bad**: "Why now? Because we just raised funding and have a great team."

**Good**: "Why now? Because adaptive content generation has dropped from five dollars per student to fifty cents per student, making personalization economically viable for schools."

**Lesson**: Why Now must be external (technology, policy, market shifts).

---

### Pitfall 6: No Exclusions in ICP
**Bad**: "We serve all K-12 schools and teachers in any subject."

**Good**: "We serve grades six through eight math teachers in charter schools with one-to-one devices. We exclude elementary grades K through five, high school, districts without devices, and schools using mastery-based progression already."

**Lesson**: Exclusions show focus and build credibility.

---

### Pitfall 7: Quotes as Proof
**Bad**: "One teacher said, 'This is amazing!' Another said, 'My students are so engaged!'"

**Good**: "In a four-week pilot with two teachers and sixty students, time-to-proficiency decreased from thirty days to twenty-one days (one point four times faster) on released state fraction items."

**Lesson**: Quotes are nice. Data is proof.

---

### Pitfall 8: Long Assumption Lists
**Bad**: Lists twenty assumptions with no testing plans.

**Good**: Lists three to five key assumptions, each with a one-week test and concrete mitigation.

**Lesson**: Prioritize. Test the few that matter most.

---

## Quick Reference: Great vs Needs Work

| Section | Great | Needs Work |
|---------|-------|------------|
| **One-liner** | Specific ICP, concrete pain, measurable outcome, clear approach | Generic audience, vague problem, unmeasurable goal, buzzwords |
| **ICP** | Grade/subject/setting specified, school category named, inclusion AND exclusion rules | "All schools," no school category, no exclusions, fuzzy rules |
| **Strategic Narrative** | External Why Now, concrete Status Quo, measurable Promised Land, specific Proof | Internal Why Now, vague problems, no metrics, hand-wavy proof |
| **Distribution** | Specific platform, concrete access path, blockers addressed | Vague platform, no clear access, blockers ignored |
| **Measurable Outcome** | Learning metric, numeric target, thirty-day window, auditable plan, clear thresholds | Engagement metric, vague target, long window, subjective measurement |
| **Assumptions** | Three to five key ones, each with one-week test and mitigation | Long list, no tests, no mitigations |

---

## Final Advice

When in doubt:
1. **Be specific**: Names, numbers, timeframes
2. **Be measurable**: Numeric targets, objective instruments
3. **Be clear**: Plain English, complete sentences
4. **Be honest**: Include exclusions and real risks
5. **Be evidence-based**: Data over anecdotes

Use these examples as your guide. Quote them when coaching founders. Compare their drafts against these patterns.
